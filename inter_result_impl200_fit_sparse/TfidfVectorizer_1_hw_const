{'description': 'Convert a collection of raw documents to a matrix of TF-IDF features.',
 'allOf': [{'type': 'object',
            'required': ['input',
                         'encoding',
                         'decode_error',
                         'strip_accents',
                         'lowercase',
                         'preprocessor',
                         'tokenizer',
                         'analyzer',
                         'stop_words',
                         'ngram_range',
                         'max_df',
                         'min_df',
                         'max_features',
                         'vocabulary',
                         'binary',
                         'dtype',
                         'norm',
                         'use_idf',
                         'smooth_idf',
                         'sublinear_tf'],
            'relevantToOptimizer': ['analyzer',
                                    'ngram_range',
                                    'max_df',
                                    'min_df',
                                    'binary',
                                    'norm',
                                    'use_idf',
                                    'smooth_idf',
                                    'sublinear_tf'],
            'additionalProperties': False,
            'properties': {'input': {'enum': ['filename', 'file', 'content'], 'default': 'content'},
                           'encoding': {'type': 'string', 'default': 'utf-8'},
                           'decode_error': {'enum': ['strict', 'ignore', 'replace'], 'default': 'strict'},
                           'strip_accents': {'enum': ['ascii', 'unicode', None], 'default': None},
                           'lowercase': {'type': 'boolean', 'default': True},
                           'preprocessor': {'anyOf': [{'laleType': 'callable', 'forOptimizer': False},
                                                      {'enum': [None]}],
                                            'default': None},
                           'tokenizer': {'anyOf': [{'laleType': 'callable', 'forOptimizer': False}, {'enum': [None]}],
                                         'default': None},
                           'analyzer': {'anyOf': [{'enum': ['word', 'char', 'char_wb']},
                                                  {'laleType': 'callable', 'forOptimizer': False}],
                                        'default': 'word'},
                           'stop_words': {'anyOf': [{'enum': [None]}, {'type': 'array'}, {'type': 'string'}],
                                          'default': None},
                           'token_pattern': {'type': 'string', 'default': '(?u)\\b\\w\\w+\\b'},
                           'ngram_range': {'default': (1, 1),
                                           'anyOf': [{'type': 'array',
                                                      'laleType': 'tuple',
                                                      'minItemsForOptimizer': 2,
                                                      'maxItemsForOptimizer': 2,
                                                      'items': {'type': 'integer',
                                                                'minimumForOptimizer': 1,
                                                                'maximumForOptimizer': 3},
                                                      'forOptimizer': False},
                                                     {'enum': [(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)]}]},
                           'max_df': {'anyOf': [{'description': 'float in range [0.0, 1.0]',
                                                 'type': 'number',
                                                 'minimum': 0.0,
                                                 'maximum': 1.0,
                                                 'minimumForOptimizer': 0.8,
                                                 'maximumForOptimizer': 0.9,
                                                 'distribution': 'uniform'},
                                                {'type': 'integer', 'forOptimizer': False}],
                                      'default': 1.0},
                           'min_df': {'anyOf': [{'description': 'float in range [0.0, 1.0]',
                                                 'type': 'number',
                                                 'minimum': 0.0,
                                                 'maximum': 1.0,
                                                 'minimumForOptimizer': 0.0,
                                                 'maximumForOptimizer': 0.1,
                                                 'distribution': 'uniform'},
                                                {'type': 'integer', 'forOptimizer': False}],
                                      'default': 1},
                           'max_features': {'anyOf': [{'type': 'integer', 'minimum': 1, 'maximumForOptimizer': 10000},
                                                      {'enum': [None]}],
                                            'default': None},
                           'vocabulary': {'description': 'XXX TODO XXX, Mapping or iterable, optional',
                                          'anyOf': [{'type': 'object'}, {'enum': [None]}],
                                          'default': None},
                           'binary': {'type': 'boolean', 'default': False},
                           'dtype': {'description': 'XXX TODO XXX, type, optional',
                                     'type': 'string',
                                     'default': 'float64'},
                           'norm': {'enum': ['l1', 'l2', None], 'default': 'l2'},
                           'use_idf': {'type': 'boolean', 'default': True},
                           'smooth_idf': {'type': 'boolean', 'default': True},
                           'sublinear_tf': {'type': 'boolean', 'default': False}}},
           {'description': "tokenizer, only applies if analyzer == 'word'",
            'anyOf': [{'type': 'object', 'properties': {'analyzer': {'enum': ['word']}}},
                      {'type': 'object', 'properties': {'tokenizer': {'enum': [None]}}}]},
           {'description': "stop_words can be a list only if analyzer == 'word'",
            'anyOf': [{'type': 'object', 'properties': {'stop_words': {'not': {'type': 'array'}}}},
                      {'type': 'object', 'properties': {'analyzer': {'enum': ['word']}}}]}]}
